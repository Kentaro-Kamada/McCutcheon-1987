LEM: log-linear and event history analysis with missing data.
Developed by Jeroen Vermunt (c), Tilburg University, The Netherlands.
Version 1.0 (September 18, 1997).


*** INPUT ***

  lat 1
  man 5
  dim 3 2 3 2 3 2
  lab X G P A C U
  mod G 
      X|G eq2
      P|X eq2
      A|X eq2
      C|X eq2
      U|X eq2
  dat [419 71 35 25 2 5 270 42 25 16 4 5 23 6 4 2 1 0 43 9 9 3 2 2 26 1 3 2 0 0 85 13 23 12 6 8 117 34 14 19 3 5 95 23 10 14 3 2 7 3 1 1 0 0 19 2 1 1 2 1 6 3 0 1 0 0 30 9 9 7 1 4]
  des [0 0 0 0 4 4
       2 3 0 2 3 0 0 0 0
       1 0 1 0 -1 0
       0 0 -1 0 0 0 0 0 0
       0 -1 0 0 0 0]
  sta A|X [.5 .5 .5 .5 .0 1.0]
  sta C|X [.5 .5 .0 .5 .5 .5 .5 .5 .5]
  sta U|X [1.0 .0 .5 .5 .5 .5]
  see 123


*** STATISTICS ***

  Number of iterations = 156
  Converge criterion   = 0.0000009601
  Seed random values   = 123

  X-squared            = 51.6769 (0.6024)
  L-squared            = 52.2970 (0.5786)
  Cressie-Read         = 50.9812 (0.6289)
  Dissimilarity index  = 0.0472
  Degrees of freedom   = 55
  Log-likelihood       = -4860.01380
  Number of parameters = 16 (+1)
  Sample size          = 1649.0
  BIC(L-squared)       = -355.1389
  AIC(L-squared)       = -57.7030
  BIC(log-likelihood)  = 9838.5544
  AIC(log-likelihood)  = 9752.0276

WARNING: no information is provided on identification of parameters



*** FREQUENCIES ***

  G P A C U     observed  estimated  std. res.
  1 1 1 1 1     419.000    410.230      0.433
  1 1 1 1 2      71.000     61.537      1.206
  1 1 1 2 1      35.000     34.622      0.064
  1 1 1 2 2      25.000     27.172     -0.417
  1 1 1 3 1       2.000      3.356     -0.740
  1 1 1 3 2       5.000      6.214     -0.487
  1 1 2 1 1     270.000    277.292     -0.438
  1 1 2 1 2      42.000     43.322     -0.201
  1 1 2 2 1      25.000     26.095     -0.214
  1 1 2 2 2      16.000     18.908     -0.669
  1 1 2 3 1       4.000      3.685      0.164
  1 1 2 3 2       5.000      4.556      0.208
  1 2 1 1 1      23.000     27.596     -0.875
  1 2 1 1 2       6.000      4.140      0.914
  1 2 1 2 1       4.000      2.329      1.095
  1 2 1 2 2       2.000      1.828      0.127
  1 2 1 3 1       1.000      0.226      1.630
  1 2 1 3 2       0.000      0.418     -0.647
  1 2 2 1 1      43.000     39.147      0.616
  1 2 2 1 2       9.000      9.755     -0.242
  1 2 2 2 1       9.000      9.361     -0.118
  1 2 2 2 2       3.000      3.810     -0.415
  1 2 2 3 1       2.000      3.505     -0.804
  1 2 2 3 2       2.000      1.394      0.514
  1 3 1 1 1      26.000     24.096      0.388
  1 3 1 1 2       1.000      3.615     -1.375
  1 3 1 2 1       3.000      2.034      0.678
  1 3 1 2 2       2.000      1.596      0.320
  1 3 1 3 1       0.000      0.197     -0.444
  1 3 1 3 2       0.000      0.365     -0.604
  1 3 2 1 1      85.000     78.834      0.694
  1 3 2 1 2      13.000     23.421     -2.153
  1 3 2 2 1      23.000     24.744     -0.351
  1 3 2 2 2      12.000      8.858      1.056
  1 3 2 3 1       6.000     10.157     -1.304
  1 3 2 3 2       8.000      3.586      2.331
  2 1 1 1 1     117.000    126.561     -0.850
  2 1 1 1 2      34.000     38.032     -0.654
  2 1 1 2 1      14.000     14.679     -0.177
  2 1 1 2 2      19.000     16.793      0.538
  2 1 1 3 1       3.000      2.074      0.643
  2 1 1 3 2       5.000      3.840      0.592
  2 1 2 1 1      95.000     86.143      0.954
  2 1 2 1 2      23.000     26.003     -0.589
  2 1 2 2 1      10.000     10.883     -0.268
  2 1 2 2 2      14.000     11.399      0.770
  2 1 2 3 1       3.000      1.910      0.789
  2 1 2 3 2       2.000      2.693     -0.422
  2 2 1 1 1       7.000      8.514     -0.519
  2 2 1 1 2       3.000      2.558      0.276
  2 2 1 2 1       1.000      0.987      0.013
  2 2 1 2 2       1.000      1.130     -0.122
  2 2 1 3 1       0.000      0.140     -0.374
  2 2 1 3 2       0.000      0.258     -0.508
  2 2 2 1 1      19.000     13.416      1.525
  2 2 2 1 2       2.000      4.293     -1.107
  2 2 2 2 1       1.000      3.560     -1.357
  2 2 2 2 2       1.000      1.711     -0.543
  2 2 2 3 1       2.000      1.340      0.570
  2 2 2 3 2       1.000      0.585      0.542
  2 3 1 1 1       6.000      7.434     -0.526
  2 3 1 1 2       3.000      2.234      0.513
  2 3 1 2 1       0.000      0.862     -0.929
  2 3 1 2 2       1.000      0.986      0.014
  2 3 1 3 1       0.000      0.122     -0.349
  2 3 1 3 2       0.000      0.226     -0.475
  2 3 2 1 1      30.000     28.320      0.316
  2 3 2 1 2       9.000      9.291     -0.095
  2 3 2 2 1       9.000      9.271     -0.089
  2 3 2 2 2       7.000      3.551      1.831
  2 3 2 3 1       1.000      3.809     -1.439
  2 3 2 3 2       4.000      1.392      2.210



*** PSEUDO R-SQUARED MEASURES ***

* P(X|G) *
                         baseline   fitted   R-squared
  entropy                 0.9632    0.9517    0.0119
  qualitative variance    0.2868    0.2825    0.0150
  classification error    0.4255    0.4259   -0.0010
  -2/N*log-likelihood     1.9264    1.9030    0.0122/0.0229
  likelihood^(-2/N)       6.8645    6.7057    0.0231/0.0271

* P(P|X) *
                         baseline   fitted   R-squared
  entropy                 0.7031    0.4921    0.3001
  qualitative variance    0.1939    0.1247    0.3567
  classification error    0.2371    0.1459    0.3845
  -2/N*log-likelihood     1.4062    0.9842    0.3001/0.2967
  likelihood^(-2/N)       4.0804    2.6758    0.3442/0.4560

* P(A|X) *
                         baseline   fitted   R-squared
  entropy                 0.6930    0.5643    0.1857
  qualitative variance    0.2499    0.2010    0.1957
  classification error    0.4912    0.3323    0.3236
  -2/N*log-likelihood     1.3860    1.1286    0.1857/0.2047
  likelihood^(-2/N)       3.9988    3.0914    0.2269/0.3026

* P(C|X) *
                         baseline   fitted   R-squared
  entropy                 0.5545    0.4686    0.1551
  qualitative variance    0.1510    0.1332    0.1180
  classification error    0.1777    0.1777    0.0000
  -2/N*log-likelihood     1.1091    0.9371    0.1550/0.1467
  likelihood^(-2/N)       3.0316    2.5527    0.1580/0.2358

* P(U|X) *
                         baseline   fitted   R-squared
  entropy                 0.5177    0.2621    0.4937
  qualitative variance    0.1675    0.0905    0.4598
  classification error    0.2129    0.1333    0.3740
  -2/N*log-likelihood     1.0354    0.5242    0.4937/0.3383
  likelihood^(-2/N)       2.8163    1.6892    0.4002/0.6206



*** LOG-LINEAR PARAMETERS ***

* TABLE G [or P(G)] *

  effect           beta   exp(beta)
  G 
   1             0.4946      1.6398 
   2            -0.4946      0.6098 



*** (CONDITIONAL) PROBABILITIES ***

* P(G) *

  1              0.7289
  2              0.2711

* P(X|G) *

  1 | 1          0.6147
  1 | 2          0.4648
  2 | 1          0.2264
  2 | 2          0.3762
  3 | 1          0.1589
  3 | 2          0.1589

* P(P|X) *

  1 | 1          0.8881
  2 | 1          0.0597
  3 | 1          0.0522
  1 | 2          0.8881
  2 | 2          0.0597
  3 | 2          0.0522
  1 | 3          0.1003
  2 | 3          0.2257
  3 | 3          0.6740

* P(A|X) *

  1 | 1          0.6049
  2 | 1          0.3951
  1 | 2          0.6049
  2 | 2          0.3951
  1 | 3          0.0000 *
  2 | 3          1.0000 *

* P(C|X) *

  1 | 1          0.9498
  2 | 1          0.0502
  3 | 1          0.0000 *
  1 | 2          0.6483
  2 | 2          0.2863
  3 | 2          0.0655
  1 | 3          0.6536
  2 | 3          0.2425
  3 | 3          0.1039

* P(U|X) *

  1 | 1          1.0000 *
  2 | 1          0.0000 *
  1 | 2          0.3507
  2 | 2          0.6493
  1 | 3          0.7498
  2 | 3          0.2502



*** LATENT CLASS OUTPUT ***

          X  1    X  2    X  3
         0.5741  0.2670  0.1589
  G  1   0.7805  0.6180  0.7289
  G  2   0.2195  0.3820  0.2711
  P  1   0.8881  0.8881  0.1003
  P  2   0.0597  0.0597  0.2257
  P  3   0.0522  0.0522  0.6740
  A  1   0.6049  0.6049  0.0000
  A  2   0.3951  0.3951  1.0000
  C  1   0.9498  0.6483  0.6536
  C  2   0.0502  0.2863  0.2425
  C  3   0.0000  0.0655  0.1039
  U  1   1.0000  0.3507  0.7498
  U  2   0.0000  0.6493  0.2502

E = 0.1330, lambda = 0.6877
